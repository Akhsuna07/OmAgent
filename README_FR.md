<div>
    <h1> <img src="docs/images/logo.png" height=33 align="texttop">OmAgent</h1>
</div>

<p align="center">
  <img src="docs/images/icon.png" width="300"/>
</p>

<p align="center">
    <a>French</a> | <a href="README_ZH.md">‰∏≠Êñá</a> | <a href="README_JP.md">Êó•Êú¨Ë™û</a> | Fran√ßais
</p>


## üóìÔ∏è Mises √† jour
* 20/09/2024 : Notre article a √©t√© accept√© par EMNLP 2024. Rendez-vous √† Miami !üèù
* 04/07/2024 : Le projet open-source OmAgent a √©t√© d√©voil√©. üéâ
* 24/06/2024 : [L'article de recherche OmAgent a √©t√© publi√©.](https://arxiv.org/abs/2406.16620)




## üìñ Introduction
OmAgent est un syst√®me d'agent intelligent multimodal sophistiqu√©, d√©di√© √† exploiter la puissance des grands mod√®les de langage multimodaux et d'autres algorithmes multimodaux pour accomplir des t√¢ches int√©ressantes. Le projet OmAgent comprend un cadre d'agent intelligent l√©ger, omagent_core, con√ßu m√©ticuleusement pour relever les d√©fis multimodaux. Gr√¢ce √† ce cadre, nous avons construit un syst√®me de compr√©hension vid√©o de longue dur√©e complexe‚ÄîOmAgent. Naturellement, vous avez la libert√© de l'utiliser pour r√©aliser vos id√©es innovantes.  
OmAgent se compose de trois composants principaux :  
- **Video2RAG** : L'id√©e derri√®re ce composant est de transformer la compr√©hension de longues vid√©os en une t√¢che multimodale RAG. L'avantage de cette approche est qu'elle transcende les limitations impos√©es par la longueur des vid√©os ; cependant, l'inconv√©nient est que ce pr√©traitement peut entra√Æner une perte substantielle de d√©tails vid√©o.  
- **DnCLoop** : Inspir√© par le paradigme algorithmique classique de Diviser pour R√©gner, nous avons con√ßu une logique de traitement de t√¢ches g√©n√©rale r√©cursive. Cette m√©thode affine de mani√®re it√©rative les probl√®mes complexes en un arbre de t√¢ches, transformant finalement les t√¢ches complexes en une s√©rie de t√¢ches plus simples et solvables.  
- **Outil Rewinder** : Pour pallier la perte d'information dans le processus Video2RAG, nous avons con√ßu un outil appel√© "barre de progression", Rewinder, que les agents peuvent utiliser de mani√®re autonome. Cela permet aux agents de revisiter tous les d√©tails d'une vid√©o, leur permettant de rechercher les informations n√©cessaires.  

<p align="center">
  <img src="docs/images/OmAgent.png" width="700"/>
</p>

Pour plus de d√©tails, consultez notre article **[OmAgent : Un cadre d'agent multimodal pour la compr√©hension vid√©o complexe avec Diviser pour R√©gner](https://arxiv.org/abs/2406.16620)**

## üõ†Ô∏è Comment installer
- python >= 3.10
- Installer omagent_core
  ```bash
  cd omagent-core
  pip install -e .



Autres d√©pendances
```
cd ..
pip install -r requirements.txt
```
## üöÄ D√©marrage rapide

### Traitement des t√¢ches g√©n√©rales
1. Cr√©ez un fichier de configuration et d√©finissez quelques variables n√©cessaires :
   ```shell
   cd workflows/general && vim config.yaml


| Nom de la configuration   | Utilisation                                                                                   |
|---------------------------|-----------------------------------------------------------------------------------------------|
| custom_openai_endpoint     | Adresse API pour appeler OpenAI GPT ou un autre MLLM, format : ```{custom_openai_endpoint}/chat/completions``` |
| custom_openai_key          | api_key fourni par le fournisseur MLLM                                                        |
| bing_api_key               | Cl√© API de Bing, utilis√©e pour la recherche web                                               |


2. Configurez ```run.py```
    ```python
    def run_agent(task):
        logging.init_logger("omagent", "omagent", level="INFO")
        registry.import_module(project_root=Path(__file__).parent, custom=["./engine"])
        bot_builder = Builder.from_file("workflows/general") # R√©pertoire de configuration du workflow de traitement des t√¢ches g√©n√©rales
        input = DnCInterface(bot_id="1", task=AgentTask(id=0, task=task))
    
        bot_builder.run_bot(input)
        return input.last_output
    
    
    if __name__ == "__main__":
        run_agent("Votre requ√™te") # Entrez votre requ√™te
    ```
3. D√©marrez OmAgent en ex√©cutant ```python run.py```.

### T√¢che de compr√©hension vid√©o
#### Pr√©paration de l'environnement
- **```Optionnel```** OmAgent utilise par d√©faut Milvus Lite comme base de donn√©es vectorielle pour stocker des donn√©es vectorielles. Si vous souhaitez utiliser le service complet de Milvus, vous pouvez le d√©ployer via [base de donn√©es vectorielle milvus](https://milvus.io/docs/install_standalone-docker.md) en utilisant docker. La base de donn√©es vectorielle est utilis√©e pour stocker les vecteurs de fonctionnalit√©s vid√©o et r√©cup√©rer des vecteurs pertinents en fonction des requ√™tes afin de r√©duire le calcul de MLLM. Vous n'avez pas install√© docker ? Consultez le [guide d'installation de docker](https://docs.docker.com/get-docker/).
    ```shell
       # T√©l√©chargez le script de d√©marrage de milvus
       curl -sfL https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh -o standalone_embed.sh
       # D√©marrez milvus en mode autonome
       bash standalone_embed.sh start
    ```
    Remplissez les informations de configuration pertinentes apr√®s le d√©ploiement ```workflows/video_understanding/config.yml```  
    
- **```Optionnel```** Configurez l'algorithme de reconnaissance faciale. L'algorithme de reconnaissance faciale peut √™tre utilis√© comme un outil par l'agent, mais il est facultatif. Vous pouvez d√©sactiver cette fonctionnalit√© en modifiant le fichier de configuration ```workflows/video_understanding/tools/video_tools.json``` et en supprimant la section FaceRecognition. La base de donn√©es de reconnaissance faciale par d√©faut est stock√©e dans le r√©pertoire ```data/face_db```, avec diff√©rents dossiers correspondant √† diff√©rentes personnes.
- **```Optionnel```** Service de d√©tection de vocabulaire ouvert (ovd), utilis√© pour am√©liorer la capacit√© d'OmAgent √† reconna√Ætre divers objets. Les outils ovd d√©pendent de ce service, mais il est facultatif. Vous pouvez d√©sactiver les outils ovd en suivant ces √©tapes. Supprimez ce qui suit dans ```workflows/video_understanding/tools/video_tools.json```
    ```json 
       {
            "name": "ObjectDetection",
            "ovd_endpoint": "$<ovd_endpoint::http://host_ip:8000/inf_predict>",
            "model_id": "$<ovd_model_id::OmDet-Turbo_tiny_SWIN_T>"
       }
    ```
  
  Si vous utilisez les outils ovd, nous utilisons [OmDet](https://github.com/om-ai-lab/OmDet/tree/main) √† titre de d√©monstration.
  1. Installez OmDet et son environnement selon le [guide d'installation d'OmDet](https://github.com/om-ai-lab/OmDet/blob/main/install.md).
  2. Installez les exigences pour transformer l'inf√©rence OmDet en appels API
     ```text
      pip install pydantic fastapi uvicorn
     ```
  3. Cr√©ez un fichier ```wsgi.py``` pour exposer l'inf√©rence OmDet en tant qu'API
     ```shell
      cd OmDet && vim wsgi.py
     ```
     Copiez le [code API d'inf√©rence OmDet](docs/ovd_api_doc.md) dans wsgi.py
  4. D√©marrez l'API d'inf√©rence OmDet, le port par d√©faut est 8000
     ```shell
     python wsgi.py
     ```
- T√©l√©chargez des vid√©os int√©ressantes

#### Pr√©paration de l'ex√©cution
1. Cr√©ez un fichier de configuration et d√©finissez certaines variables d'environnement n√©cessaires
   ```shell
   cd workflows/video_understanding && vim config.yaml
   
2. Configurez les adresses API et les cl√©s API pour MLLM et les outils.

| Nom de la configuration     | Utilisation                                                                                     |
|-----------------------------|-------------------------------------------------------------------------------------------------|
| custom_openai_endpoint       | Adresse API pour appeler OpenAI GPT ou un autre MLLM, format : ```{custom_openai_endpoint}/chat/completions``` |
| custom_openai_key            | api_key fournie par le fournisseur API respectif                                                |
| bing_api_key                 | Cl√© API de Bing, utilis√©e pour la recherche web                                                 |
| ovd_endpoint                 | Adresse API de l'outil ovd. Si vous utilisez OmDet, l'adresse doit √™tre ```http://host:8000/inf_predict``` |
| ovd_model_id                 | ID du mod√®le utilis√© par l'outil ovd. Si vous utilisez OmDet, l'ID du mod√®le doit √™tre ```OmDet-Turbo_tiny_SWIN_T``` |


2. Configurez ```run.py```
    ```python
    def run_agent(task):
        logging.init_logger("omagent", "omagent", level="INFO")
        registry.import_module(project_root=Path(__file__).parent, custom=["./engine"])
        bot_builder = Builder.from_file("workflows/video_understanding") # R√©pertoire de configuration du workflow pour la t√¢che de compr√©hension vid√©o
        input = DnCInterface(bot_id="1", task=AgentTask(id=0, task=task))
    
        bot_builder.run_bot(input)
        return input.last_output
    
    
    if __name__ == "__main__":
        run_agent("") # Vous serez invit√© √† entrer la requ√™te dans la console
    ```
3. D√©marrez OmAgent en ex√©cutant ```python run.py```. Entrez le chemin de la vid√©o que vous souhaitez traiter, attendez un moment, puis entrez votre requ√™te, et OmAgent r√©pondra en fonction de la requ√™te.

## üîó Travaux associ√©s
Si vous √™tes int√©ress√© par les algorithmes multimodaux, les grands mod√®les de langage et les technologies d'agents, nous vous invitons √† explorer davantage nos travaux de recherche :  
üîÜ [Comment √©valuer la g√©n√©ralisation de la d√©tection¬†? Un benchmark pour une d√©tection de vocabulaire ouvert compl√®te](https://arxiv.org/abs/2308.13177)(AAAI24)   
üè† [D√©p√¥t Github](https://github.com/om-ai-lab/OVDEval/tree/main)

üîÜ [OmDet¬†: Pr√©-entra√Ænement multi-dataset vision-langage √† grande √©chelle avec r√©seau de d√©tection multimodal](https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cvi2.12268)(IET Computer Vision)  
üè† [D√©p√¥t Github](https://github.com/om-ai-lab/OmDet)

## ‚≠êÔ∏è Citation

Si vous trouvez notre d√©p√¥t utile, veuillez citer notre article¬†:  
```angular2
@article{zhang2024omagent,
  title={OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer},
  author={Zhang, Lu and Zhao, Tiancheng and Ying, Heting and Ma, Yibo and Lee, Kyusong},
  journal={arXiv preprint arXiv:2406.16620},
  year={2024}
}
```
